---
title: "4b) Errors and power --- Torturing the *t*-test"
subtitle: "Data Simulation with Monte Carlo Methods"
author: "Marko Bachl"
institute: "University of Hohenheim"
format:
  revealjs:
      embed-resources: false
      height: 900
      width: 1600
      theme: [theme.scss]
      highlight-style: arrow-dark
      slide-number: c
      code-line-numbers: false
      history: false
      smaller: false
      title-slide-attributes:
        data-background-image: https://upload.wikimedia.org/wikipedia/commons/6/64/Uni_Hohenheim-Logo.svg
        data-background-position: "top right"
        data-background-size: auto
css: colors.css
execute:
  echo: true
---

```{r setup}
#| include: false
pacman::p_load(knitr, mgcv, tictoc, tidyverse)
theme_set(hrbrthemes::theme_ipsum_rc(base_size = 25,
                                     axis_title_size = 25,
                                     strip_text_size = 20))
```

# Errors and power --- Torturing the *t*-test, part II

## Traktandenliste

1)  Introduction & overview

2)  Monte Carlo Simulation?

3)  Proof by simulation --- The Central Limit Theorem (CLT)

4)  Errors and power --- Torturing the *t*-test

5)  Misclassification and bias --- Messages mismeasured

6)  Outlook: What's next?


## 4. Errors and power --- Torturing the *t*-test

a)  Comparison of Student's and Welch's *t*-tests

b)  *A priori* power calculation for Welch's *t*-test


## Student's *t*-test & Welch's *t*-test

-   Old school advice: Student's *t*-test for equal variances, Welch's *t*-test for unequal variances.

    -   Higher power of Student's *t*-test if assumptions hold.

-   Modern advice: Always use Welch's *t*-test.

    -   Better if assumptions are violated; not worse if assumptions hold.

    -   e.g., Delacre, M., Lakens, D., & Leys, C. (2017). Why Psychologists Should by Default Use Welch's *t*-test Instead of Student's *t*-test. *International Review of Social Psychology, 30*(1). <https://doi.org/10.5334/irsp.82>

-   For those who don't care about *t*-tests: Idea also applies to heteroskedasticity-consistent standard errors.

## Second Simulation study: Power {.smaller}

1.  Question: What is the goal of the simulation?

    -   Comparison of Student's and Welch's *t*-tests

2.  Quantities of interest: What is measured in the simulation?

    -   Power ($1 - \beta$) of the tests

3.  Evaluation strategy: How are the quantities assessed?

    -   Comparison of statistical power to reject the Null hypothesis if the alternative hypothesis is true

4.  Conditions: Which characteristics of the data generating model will be varied?

    -   Sample size, effect size (mean difference)

5.  Data generating model: How are the data simulated?

    -   Random draws from normal distributions with different group means and different overall sample sizes, but euqal group sizes and standard deviations

## Additional practical considerations

-   How to adapt the simulation function to include different group means

-   How to adapt the experimental conditions

    - to include different group means and different total sample sizes
    
    - to include many different levels of the total sample size

## Adapt the simulation function

:::: {.columns}

::: {.column width="45%"}
- Include new argument `M_diff`

    - Mean of the treatment condition; equals mean difference, as mean of the control is fixed at 0.
    
    - Mean difference equals standardized effect size *d* if `SDR`, and, consequently, both group standard deviations, are 1.
:::

::: {.column width="55%"}
```{r}
sim_ttest = function(n = 200, GR = 1, SDR = 1, M_diff = 0) {
  n1 = round(n / (GR + 1))
  n2 = round(n1 * GR)
  sd1 = 1
  sd2 = sd1 * SDR # sd2/sd1
  g1 = rnorm(n = n1, mean = 0, sd = sd1) # control
  g2 = rnorm(n = n2, mean = M_diff, sd = sd2) # treatment
  welch = t.test(g1, g2)$p.value
  student = t.test(g1, g2, var.equal = TRUE)$p.value
  res = list("Welch" = welch, "Student" = student)
  res
}
sim_ttest(n = 100, M_diff = 0.3)
```
:::

::::

## Adapt the experimental conditions

:::: {.columns}

::: {.column width="50%"}
-   `GR` and `SDR` are now fixed at 1.

-   New between-simulation factors: `n` and `M_diff`
:::

::: {.column width="50%"}
```{r}
conditions = expand_grid(
  GR = 1, 
  SDR = 1,
  n = c(100, 200, 300),
  M_diff = c(0.2, 0.4, 0.6)) %>% 
  rowid_to_column(var = "condition")
conditions
```

:::

::::

## Run simulation experiment

```{r}
#| cache: true
set.seed(326)
i = 10000
tic()
sims = map_dfr(1:i, ~ conditions) %>% # each condition i times
  rowid_to_column(var = "sim") %>% # within simulation comparison
  rowwise() %>%
  mutate(p.value = list(sim_ttest(n = n, M_diff = M_diff))) %>% 
  unnest_longer(p.value, indices_to = "method")
toc()
```

## Check results: Simulation *v* analytical solution

:::: {.columns}

::: {.column width="50%"}
```{r}
sims %>% 
  filter(M_diff == 0.4 & method == "Student") %>% 
  group_by(n, M_diff, method) %>% 
  summarise(P_p05 = mean(p.value < 0.05)) %>% 
  ungroup() %>% 
  mutate(across(where(is.numeric), round, 3))
```

:::

::: {.column width="50%"}
```{r}
power.t.test(n = c(100, 200, 300)/2,
             delta = 0.4, sd = 1,
             sig.level = 0.05)
```

:::

::::

- Close enough for government work

## Results: Power comparison

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-width: 12
#| fig-height: 5
sims %>% 
  group_by(n, M_diff, method) %>% 
  summarise(P_p05 = mean(p.value < 0.05)) %>% 
  ggplot(aes(factor(n), P_p05, color = method, group = method)) +
  geom_point(position = position_dodge(width = 0.3), size = 3) + 
  geom_line(position = position_dodge(width = 0.3), size = 1.5) +
  facet_wrap(vars(M_diff), labeller = label_both) +
  labs(x = "n", y = "Power")
```

## Summary of the *t*-test comparison

- Student's *t*-test fails the nominal false discovery rate if variances *and* group sizes are unequal (simulation study 1).

- Welch's *t*-test has similar statistical power as Student's *t*-test if the assumption of equal group variances holds (simulation study 2).

- We should always use Welch's *t*-test as a default.

- For those who don't use *t*-tests: We probably should also use heteroskedasticity-consistent standard errors as default in OLS linear models.


# Questions?

# *A priori* power calculation for Welch's *t*-test

## *a priori* Power calculation for Welch's *t*-test

- So far we have learnt that we should prefer Welch's *t*-test over Student's *t*-test when comparing two group means. 

- We also know that different group sizes and group standard deviations influence statistical power.

- However, the standard options for *a priori* power calculation often do not allow for varying group sizes *and* varying group *SD*s.

- This is a good segue to using Monte Carlo simulation experiments for *a priori* power calculation with a (seemingly) simple example.

## Using a model fitted to simulated data as power calculator

- We start with a simple example: The relationship between sample size and statistical power.

- For now, we assume equal group sample sizes and group standard deviations.

- We also limit the example to one effect size.

## Calculator: Adapt the simulation function

:::: {.columns}

::: {.column width="45%"}
- We no longer need Student's *t*-test.
    - The output is simply the *p*-value of Welch's *t*-test.

- Beyond that, the simulation function stays the same.
    
:::

::: {.column width="55%"}
```{r}
sim_ttest = function(n = 200, GR = 1, SDR = 1, M_diff = 0) {
  n1 = round(n / (GR + 1))
  n2 = round(n1 * GR)
  sd1 = 1
  sd2 = sd1 * SDR # sd2/sd1
  g1 = rnorm(n = n1, mean = 0, sd = sd1)
  g2 = rnorm(n = n2, mean = M_diff, sd = sd2)
  res = t.test(g1, g2)$p.value
  return(res)
}
sim_ttest(n = 300, M_diff = 0.4)
```
:::

::::

## Calculator: Adapt the experimental conditions {.smaller}

:::: {.columns}

::: {.column width="50%"}
Option 1: more fine-grained factor `n`

```{r}
conditions = expand_grid(GR = 1, 
                         SDR = 1,
                         n = seq(from = 50, to = 500, by = 10),
                         M_diff = 0.4) %>% 
  rowid_to_column(var = "condition")
i = 250
conditions = map_dfr(1:i, ~ conditions) %>%
  rowid_to_column(var = "sim")
conditions
```
:::

::: {.column width="50%"}
Option 2: many random draws of `n` from a distribution

```{r}
i = 250*46 # same total number of simulations
set.seed(86)
conditions = tibble(sim = 1:i,
                    GR = 1, 
                    SDR = 1,
                    n = sample(x = 50:500, size = i, replace = TRUE),
                    M_diff = 0.4)
conditions
```

:::

::::

## Calculator: Run simulation

```{r}
#| cache: true
tic()
sims = conditions %>% 
  rowwise() %>%
  mutate(p.value = sim_ttest(n = n, M_diff = M_diff, GR = GR, SDR = SDR))
toc()
```


## Calculator: Visual results

```{r}
#| output-location: column
sims %>% 
  mutate(p05 = as.integer(p.value < 0.05)) %>% 
  ggplot(aes(n, p05)) +
  geom_smooth(method = "gam", 
              formula = y ~ s(x, bs = "cr"), 
              method.args = list(family = "binomial")) + 
  labs(y = "Power")
```

## Calculator: Predict results for any `n`

:::: {.columns}

Fit (non-linear) model (e.g., `mgcv::gam`)

```{r}
power_mod = sims %>% 
  mutate(p05 = as.integer(p.value < 0.05)) %>%
  gam(p05 ~ s(n, bs = "cr"), family = binomial, data = .)
```

::: {.column width="50%"}
Predict power for any `n` (within range of simulated values!)

```{r}
tibble(n = c(120, 256, 404)) %>% 
  mutate(power = predict(power_mod, newdata = .,
                         type = "response"))
```
:::

::: {.column width="50%"}
Check against analytical solution

```{r}
power.t.test(n = c(120, 256, 404)/2,
             delta = 0.4, sd = 1, 
             sig.level = 0.05)
```

:::

::::

## Calculator: Summary

- (Trivial) result: Larger sample size, more power

- General take-away: Using many values instead of few discrete factor levels can be an interesting alternative
    - if we know (assume) that there is a fixed functional relationship with the outcome
    - if we want to look at an outcome across the whole range of values
    - if we want to make predictions across the range of values
    - if we evaluate models which need more computational resources
    - Not so helpful if discretized after the simulation.

# Questions?

# *A priori* power calculation for Welch's *t*-test

## Getting started: Some preliminary thoughts

- We know that effect size and total sample size are positively related to statistical power, so we are not that interested in their effects.

- We know less about the role of group sample sizes and group standard deviations differences, so this is what we want to investigate in the simulation.

- We have to think more about how we define the effect size.
    - This is generally one of the harder tasks in *a priori* power calculation
    - and it becomes more complicated if simple "mean divided by SD" rules of thumb get more ambiguous.

## Third Simulation study: Power (again) {.smaller}

1.  Question: What is the goal of the simulation?

    -   Understand roles of group sample sizes and group standard deviations for statistical power

2.  Quantities of interest: What is measured in the simulation?

    -   Power ($1 - \beta$) of the designs using Welch's *t*-test

3.  Evaluation strategy: How are the quantities assessed?

    -   Comparison of statistical power to reject the Null hypothesis if the alternative hypothesis is true

4.  Conditions: Which characteristics of the data generating model will be varied?

    -   Group sample sizes, group standard deviations

5.  Data generating model: How are the data simulated?

    -   Random draws from normal distributions with different group sizes and standard deviations, but fixed group mean differences and overall sample size.

## First try: Adapt the simulation function

:::: {.columns}

::: {.column width="45%"}
- We use the same simulation function as before.
    
:::

::: {.column width="55%"}
```{r}
sim_ttest = function(n = 200, GR = 1, SDR = 1, M_diff = 0) {
  n1 = round(n / (GR + 1))
  n2 = round(n1 * GR)
  sd1 = 1
  sd2 = sd1 * SDR # sd2/sd1
  g1 = rnorm(n = n1, mean = 0, sd = sd1)
  g2 = rnorm(n = n2, mean = M_diff, sd = sd2)
  res = t.test(g1, g2)$p.value
  return(res)
}
sim_ttest(n = 300, M_diff = 0.3, GR = 0.5)
```
:::

::::


## First try: Adapt the experimental conditions

:::: {.columns}

::: {.column width="50%"}
-   Total sample size `n = 300` and effect size `M_diff = 0.3` are now fixed.

-   New between-simulation factors: `GR` and `SDR`

:::

::: {.column width="50%"}
```{r}
conditions = expand_grid(GR = c(0.5, 1, 2), 
                         SDR = c(0.5, 1, 2),
                         n = 300,
                         M_diff = 0.3) %>% 
  rowid_to_column(var = "condition") %>% 
  mutate(
    n1 = round(n / (GR + 1)),
    n2 = round(n1 * GR),
    sd1 = 1,
    sd2 = sd1 * SDR
    ) 
conditions %>% select(1:5)
```

:::

::::

## This has some implications

- For effect size interpretation:

    - If the *SD* ratio is not equal to 1, the pooled *SD* is not equal to 1 and the mean difference is not in units of the pooled *SD* (as in traditional *d*).
    
    - Instead, it is the difference in *SD* of the control group units.
    
    - Sensible definition for intervention trials with passive control group: Estimate of the population variation without a intervention.
    
    - Maybe less sensible in other designs with more active controls (e.g., typical media effects experimental studies).
    
## This has some implications

- For substantive interpretation:

    - `SDR = 0.5` assumes a treatment which not only increases the level of the outcome by `M_diff`, but also *halves* the variation in the outcome.
    
    - `SDR = 2` assumes a treatment which not only increases the level of the outcome by `M_diff`, but also *doubles* the variation in the outcome.
    
    - Heterogeneous treatment effects!

## First try: Run simulation experiment

```{r}
#| cache: true
set.seed(7913)
i = 10000
tic()
sims = map_dfr(1:i, ~ conditions) %>% # each condition 10,000 times
  rowid_to_column(var = "sim") %>%
  rowwise() %>%
  mutate(p.value = sim_ttest(n = n, M_diff = M_diff, GR = GR, SDR = SDR))
toc()
```


## First try: Results

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-width: 12
#| fig-height: 5
sims %>% 
  group_by(GR, SDR) %>% 
  summarise(P_p05 = mean(p.value < 0.05)) %>% 
  ggplot(aes(factor(GR), P_p05, group = 1)) + 
  geom_point() + geom_line() +
  facet_wrap(vars(SDR), labeller = label_both) +
  labs(x = "Group size ratio", y = "Power")
```

## First try: Results

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-width: 12
#| fig-height: 5
sims %>% 
  group_by(n, M_diff, GR, SDR, n1, n2, sd1, sd2) %>% 
  summarise(P_p05 = mean(p.value < 0.05), .groups = "drop") %>% 
  mutate(implies = str_glue("n_c = {n1}, n_t = {n2}, sd_c = {round(sd1, 2)}, sd_t = {round(sd2, 2)}")) %>% 
  mutate(implies = fct_reorder(implies, P_p05)) %>% 
  ggplot(aes(P_p05, implies)) + geom_point(size = 3) +
  labs(x = "Power", y = "Condition") +
  theme(axis.text.y = element_text(size = 20))
```

## First try: Summary

- Larger *SD* in the treatment group reduces statistical power, smaller *SD* in the treatment group increases power.

- Optimal sample size plan: Larger sample size for group with larger *SD*

- If group *SD*s are equal, most power with equal group sample sizes.

## Second try: Bring back a pooled *SD* of 1

- We might not be satisfied with the trivial result that more variation decreases power.

- We might be interested in a more conventional *d*-style scaling of the effect size.

- We want a pooled SD of 1 while retaining the *SD* ratio design.

## Wikipedia to the rescue

[Standard deviations of non-overlapping (X ∩ Y = ∅) sub-populations can be aggregated as follows if the size (actual or relative to one another) and means of each are known:](https://en.wikipedia.org/wiki/Pooled_variance#Population-based_statistics)


$$
\sigma_{X\cup Y} = \sqrt{ \frac{N_X \sigma_X^2 + N_Y \sigma_Y^2}{N_X + N_Y} + \frac{N_X N_Y}{(N_X+N_Y)^2}(\mu_X - \mu_Y)^2 }
$$
and

$$
\sigma_Y = SDR * \sigma_X
$$

## Second try: Adapt the simulation function

:::: {.columns}

::: {.column width="45%"}
- We add a new argument, `pooled_sd`.

- `sd1` is computed based on `pooled_sd`, `GR` (via `n1` and `n2`), `SDR`, and `M_diff`.
    
:::

::: {.column width="55%"}
```{r}
sim_ttest = function(n = 200, pooled_sd = 1, GR = 1, SDR = 1, M_diff = 0) {
  n1 = round(n / (GR + 1))
  n2 = round(n1 * GR)
  sd1 = sqrt(
    ((pooled_sd^2 - ((n1 * n2) / (n1 + n2)^2) * M_diff^2) * (n1 + n2)) / 
      (n1 + n2 * SDR^2)
    )
  sd2 = sd1 * SDR # sd2/sd1
  g1 = rnorm(n = n1, mean = 0, sd = sd1)
  g2 = rnorm(n = n2, mean = M_diff, sd = sd2)
  res = t.test(g1, g2)$p.value
  return(res)
}
sim_ttest(n = 300, M_diff = 0.3, GR = 0.5)
```
:::

::::

## Second try: Adapt the experimental conditions

:::: {.columns}

::: {.column width="50%"}
-   Total sample size `n = 300` and effect size `M_diff = 0.3` are now fixed.

- In addition, `pooled_sd` is fixed at 1, returning to a *d*-style interpretation of the effect size.

-   Between-simulation factors: `GR` and `SDR`

:::

::: {.column width="50%"}
```{r}
conditions = expand_grid(GR = c(0.5, 1, 2), 
                         SDR = c(0.5, 1, 2),
                         pooled_sd = 1,
                         n = 300,
                         M_diff = 0.3) %>% 
  rowid_to_column(var = "condition") %>% 
  mutate(
    n1 = round(n / (GR + 1)), n2 = round(n1 * GR),
    sd1 = sqrt(((pooled_sd^2 - ((n1 * n2) / (n1 + n2)^2) * M_diff^2) * (n1 + n2)) / (n1 + n2 * SDR^2)),
    sd2 = sd1 * SDR
    ) 
conditions %>% select(1:6)
```

:::

::::

## Second try: Run simulation experiment

```{r}
#| cache: true
set.seed(39)
i = 10000
tic()
sims = map_dfr(1:i, ~ conditions) %>% # each condition 10,000 times
  rowid_to_column(var = "sim") %>%
  rowwise() %>%
  mutate(p.value = sim_ttest(n = n, M_diff = M_diff, GR = GR, SDR = SDR))
toc()
```


## Second try: Results

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-width: 12
#| fig-height: 5
sims %>% 
  group_by(GR, SDR) %>% 
  summarise(P_p05 = mean(p.value < 0.05)) %>% 
  ggplot(aes(factor(GR), P_p05, group = 1)) + 
  geom_point() + geom_line() +
  facet_wrap(vars(SDR), labeller = label_both) +
  labs(x = "Group size ratio", y = "Power")
```

## Second try: Results

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| fig-width: 12
#| fig-height: 5
sims %>% 
  group_by(n, M_diff, GR, SDR, n1, n2, sd1, sd2) %>% 
  summarise(P_p05 = mean(p.value < 0.05), .groups = "drop") %>% 
  mutate(implies = str_glue("n_c = {n1}, n_t = {n2}, sd_c = {round(sd1, 2)}, sd_t = {round(sd2, 2)}")) %>% 
  mutate(implies = fct_reorder(implies, P_p05)) %>% 
  ggplot(aes(P_p05, implies)) + geom_point(size = 3) +
  labs(x = "Power", y = "Condition") +
  theme(axis.text.y = element_text(size = 20))
```

## Second try: Summary

- Fixing the pooled *SD* to 1 makes the patterns much clearer.

- If we assume different group *SD*s, we should allocate more observations in the group with more variation.

- If we assume similar group *SD*s, we should plan with similar group sample sizes.

- For planing experimental designs, it is a decision under uncertainty about treatment effect heterogeneity.

## Simulation for *a priori* power calculation

### What's the power? Don't expect an easy answer! 

- Simulation is a very flexible and powerful approach to *a priori* power calculation. We can, in principle, test many different design features and statistical methods.

- However, a meaningful power calculation (not only, but in particular with simulation methods) requires deep understanding of all involved components.

- Implementing simulation methods for *a priori* power calculation from scratch is very revealing, because it makes it very obvious how much we need to know to arrive at a sensible answer.

# Questions?

# Group exercise 2

## Exercise 2: Welch's *t*-test for count data

### Part 2: Power

- We have shown in Part 1 of the exercise that the false discovery rates of *t*-tests and Poisson regression are similar even if the true data generating process is drawing from a Poisson distribution.

- In Part 2, we want to compare the statistical power of the methods across a range of plausible scenarios.

## Exercise 2b: Power

- We want to compare the statistical power of 
    - Welch's *t*-test and
    - Poisson regression with a dummy predictor

- if the true data generating process is drawing from Poisson distributions with different rate parameters.

- If necessary: Adapt the simulation function from Exercise 2a.

- Choose a sensible set of parameter combinations for the Monte Carlo experiment. 

## Exercise 2b: Help {.scrollable}

### Expand if you need help.

### Don't expand if you want to figure it out by yourself.

### Scroll down if you need more help on later steps.

Simulation function adapted from Exercise 2a
```{r Simulation function adapted from Exercise 2a}
#| eval: false
#| code-fold: true
# We use almost the same simulation function, 
# but we omit Student's t-test, because we already showed its
# problems with unequal group sizes and group SDs
sim_ttest_glm = function(n = 200, GR = 1, lambda1 = 1, M_diff = 0) {
  n1 = round(n / (GR + 1))
  n2 = round(n1 * GR)
  lambda2 = lambda1 + M_diff
  g1 = rpois(n = n1, lambda = lambda1)
  g2 = rpois(n = n2, lambda = lambda2)
  Welch = t.test(g1, g2)$p.value
  GLM = glm(outcome ~ group,
            data = data.frame(outcome = c(g1, g2),
                              group = c(rep("g1", n1), rep("g2", n2))),
            family = poisson)
  res = lst(Welch, GLM = coef(summary(GLM))[2, 4])
  return(res)
}
```


Conditions I: Simple variation of differences between rate parameters
```{r}
#| eval: false
#| code-fold: true
# It makes sense to vary group size ratio, lambda, and M_diff
# The positive relationship between n and power is trivial
conditions = expand_grid(GR = c(0.5, 1, 2), 
                         n = 200,
                         lambda1 = c(1, 10, 20),
                         M_diff = c(0.3, 0.5, 0.8)) %>% 
  rowid_to_column(var = "condition")
```

Conditions II: Variation of differences between rate parameters standardized by $\sqrt{\lambda_1}$
```{r Conditions II}
#| eval: false
#| code-fold: true
conditions = expand_grid(GR = c(0.5, 1, 2), 
                         n = 200,
                         lambda1 = c(1, 10, 20),
                         d = c(0.2, 0.4, 0.6)) %>% # standardized mean difference.
  mutate(M_diff = d * sqrt(lambda1)) %>% # absolute mean difference
  rowid_to_column(var = "condition")
```

Result plot: Power at $p < .05$
```{r Power plot I}
#| eval: false
#| code-fold: true
sims %>% 
  group_by(GR, lambda1, M_diff, method) %>% 
  summarise(P_p05 = mean(p.value < 0.05)) %>% 
  ggplot(aes(factor(GR), P_p05, color = method, group = method)) + 
  geom_point() + geom_line() +
  facet_grid(M_diff ~ lambda1, labeller = label_both) +
  labs(x = "Group size ratio", y = "Power")
```
