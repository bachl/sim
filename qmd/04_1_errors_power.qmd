---
title: "4a) Errors and power --- torturing the *t*-test"
subtitle: "Data Simulation with Monte Carlo Methods"
author: "Marko Bachl"
institute: "University of Hohenheim"
format:
  revealjs:
      embed-resources: false
      height: 900
      width: 1600
      theme: [theme.scss]
      highlight-style: arrow-dark
      slide-number: c
      code-line-numbers: false
      history: false
      smaller: false
      title-slide-attributes:
        data-background-image: https://upload.wikimedia.org/wikipedia/commons/6/64/Uni_Hohenheim-Logo.svg
        data-background-position: "top right"
        data-background-size: auto
css: colors.css
execute:
  echo: true
---

```{r setup, include=FALSE}
pacman::p_load(knitr, tictoc, tidyverse)
theme_set(hrbrthemes::theme_ipsum_rc(base_size = 25,
                                     axis_title_size = 25,
                                     strip_text_size = 20))
```

# Errors and power --- torturing the *t*-test

## Traktandenliste

1)  Introduction & overview

2)  Monte Carlo Simulation?

3)  Proof by simulation --- the Central Limit Theorem (CLT)

4)  Errors and power --- torturing the *t*-test

5)  Misclassification and bias --- Messages mismeasured

6)  Outlook: What's next?


## 4. Errors and power --- torturing the *t*-test

a)  Comparison of Student's and Welch's *t*-tests

b)  *A priori* power calculation for Welch's *t*-test


## Student's *t*-test & Welch's *t*-test

Student's *t*-test for two independent means

$t = \frac{\bar{X}_1 - \bar{X}_2}{s_p \sqrt\frac{2}{n}}$, where $s_p = \sqrt{\frac{s_{X_1}^2+s_{X_2}^2}{2}}$ and $df = n_1 + n_2 − 2$

Welch's *t*-test for two independent means

$t = \frac{\bar{X}_1 - \bar{X}_2}{s_{\bar\Delta}}$, where ${\bar\Delta} = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$ and $df = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}\right)^2}{\frac{\left(s_1^2/n_1\right)^2}{n_1-1} + \frac{\left(s_2^2/n_2\right)^2}{n_2-1}}$

## Student's *t*-test & Welch's *t*-test

-   Old school advice: Student's *t*-test for equal variances, Welch's *t*-test for unequal variances.

    -   Higher power of Student's *t*-test if assumptions hold.

-   Modern advice: Always use Welch's *t*-test.

    -   Better if assumptions are violated; not worse if assumptions hold.

    -   e.g., Delacre, M., Lakens, D., & Leys, C. (2017). Why Psychologists Should by Default Use Welch's *t*-test Instead of Student's *t*-test. *International Review of Social Psychology, 30*(1). <https://doi.org/10.5334/irsp.82>

-   For those who don't care about *t*-tests: Idea also applies to heteroskedasticity-consistent standard errors.

## First Simulation study: False discoveries {.smaller}

1.  Question: What is the goal of the simulation?

    -   Comparison of Student's and Welch's *t*-tests

2.  Quantities of interest: What is measured in the simulation?

    -   $\alpha$ error rate of the tests

3.  Evaluation strategy: How are the quantities assessed?

    -   Comparison of empirical distribution of *p*-values with theoretical distribution under the Null hypothesis

4.  Conditions: Which characteristics of the data generating model will be varied?

    -   Group size ratio; Standard deviation ratio

5.  Data generating model: How are the data simulated?

    -   Random draws from normal distributions with equal means, but different group sizes and standard deviations

## Practical considerations

-   How does the `t.test()` work in *R*?

-   Which parameters and functions do we need for generating the data?

-   How do we implement the experimental design for the simulation study?

-   How do we run the simulation?

-   How do we assess and visualize the results?

## One simulation: Parameters

-   Two factors:

    -   Group sizes: Equal or unequal?

    -   Group standard deviations: Equal or unequal?

-   Implementation with ratios and fixed total sample size.

```{r}
set.seed(28)
n = 150 # fixed total sample size
GR = 2 # n2/n1
n1 = round(n / (GR + 1))
n2 = round(n1 * GR)
sd1 = 1
SDR = 2  # sd2/sd1
sd2 = sd1 * SDR
```

## One simulation: *t*-tests

```{r}
#| layout-ncol: 2
g1 = rnorm(n = n1, mean = 0, sd = sd1)
g2 = rnorm(n = n2, mean = 0, sd = sd2)
t.test(g1, g2) # Welch (default)
t.test(g1, g2, var.equal = TRUE) # Student
```

## Inspect the output of `t.test()`

```{r}
t.test(g1, g2) %>% str()
```

 

```{r}
t.test(g1, g2)$p.value
```

## Wrap it into a function

```{r}
sim_ttest = function(GR = 1, SDR = 1, n = 150) {
  n1 = round(n / (GR + 1))
  n2 = round(n1 * GR)
  sd1 = 1
  sd2 = sd1 * SDR
  g1 = rnorm(n = n1, mean = 0, sd = sd1)
  g2 = rnorm(n = n2, mean = 0, sd = sd2)
  welch = t.test(g1, g2)$p.value
  student = t.test(g1, g2, var.equal = TRUE)$p.value
  res = list("Welch" = welch, "Student" = student)
  return(res)
}
```

## Call the function once

```{r}
sim_ttest(GR = 2, SDR = 0.5) # Implies n1 = 50, n2 = 100, sd1 = 1, sd2 = 0.5
```

## Setup of experimental conditions

-   [`expand_grid()`](https://tidyr.tidyverse.org/reference/expand_grid.html) is the tidy implementation of [`base::expand.grid()`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/expand.grid.html).
-   Creates a data frame from all combinations of the supplied vectors.

```{r}
expand_grid(a = 1:2, b = 3:4)
```

## Setup of experimental conditions

-   Two between-simulation factors: `GR` and `SDR`

```{r}
conditions = expand_grid(GR = c(1, 2),
                         SDR = c(0.5, 1, 2)) %>% 
  rowid_to_column(var = "condition")
conditions
```

## Setup of experimental conditions

```{r}
#| echo: false
conditions %>% 
  mutate(implies = str_glue("n1 = {round(150 / (GR + 1))}, n2 = {round(150 - 150 / (GR + 1))}, sd1 = 1, sd2 = {1 * SDR}")) %>% 
  knitr::kable()
```

## Run simulation experiment

```{r}
#| cache: true
set.seed(689)
i = 10000 # number of sim runs per condition
tic() # simple way to measure wall time
sims = map_dfr(1:i, ~ conditions) %>% # each condition i times
  rowid_to_column(var = "sim") %>% # within simulation comparison
  rowwise() %>%
  mutate(p.value = list(sim_ttest(GR = GR, SDR = SDR))) %>% 
  unnest_longer(p.value, indices_to = "method")
toc() # simple way to measure wall time
```

## Inspect simulated data

```{r}
#| echo: false
sims
```

## Results: Histogram of *p*-values

  -   Distribution of *p*-values under the Null hypothesis should be $\sf{Uniform}(0, 1)$.

```{r}
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
sims %>% 
  ggplot(aes(p.value)) +
  geom_histogram(binwidth = 0.05, boundary = 0) + 
  facet_grid(method ~ GR + SDR, labeller = label_both) + 
  scale_x_continuous(breaks = c(1, 3)/4)
```

## Results: Q-Q-plot of *p*-values

-   Distribution of *p*-values under the Null hypothesis should be $\sf{Uniform}(0, 1)$.

```{r}
#| output-location: slide
#| fig-width: 16
#| fig-height: 8
sims %>% 
  ggplot(aes(sample = p.value, color = method)) +
  geom_qq(distribution = stats::qunif, size = 0.2) + 
  geom_abline(slope = 1) + 
  facet_grid(GR ~ SDR, labeller = label_both)
```

## Results: Looking at the relevant tail {.smaller}
```{r}
#| output-location: column
sims %>% 
  group_by(GR, SDR, method) %>% 
  summarise(P_p001 = mean(p.value <= 0.001),
            P_p01 = mean(p.value <= 0.01), 
            P_p05 = mean(p.value <= 0.05)) %>% 
  kable(digits = 3)
```


# Questions?

# Group exercise 2

## Exercise 2a.

-   Investigate how the amount of disparity in group sizes and standard deviations changes the results.

    -   When do the differences between the methods become neglegible?

    -   When does the false discovery rate of Student's *t*-test become really bad?

-   Adapt the experimental setup accordingly.

    -   Change or expand the levels of the GR and SDR factors.

    -   Consider changing the experimental design to make the simulation more efficient.

## Solution 2a

- TODO
